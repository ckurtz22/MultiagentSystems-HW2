%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2021 (based on sample-sigconf.tex)
%%% Prepared by Natasha Alechina and Ulle Endriss (version 2020-08-06)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.
%%% Use the first variant below for the final paper.
%%% Use the second variant below for submission.

\documentclass[sigconf]{aamas} 
%\documentclass[sigconf,anonymous]{aamas} 

%%% Load required packages here (note that many are included already).

\usepackage{balance} % for balancing columns on the final page

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2021 copyright block (do not change!)

%\setcopyright{ifaamas}
%\acmConference[AAMAS '21]{Proc.\@ of the 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2021)}{May 3--7, 2021}{London, UK}{U.~Endriss, A.~Now\'{e}, F.~Dignum, A.~Lomuscio (eds.)}
%\copyrightyear{2021}
%\acmYear{2021}
%\acmDOI{}
%\acmPrice{}
%\acmISBN{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Use this command to specify your EasyChair submission number.
%%% In anonymous mode, it will be printed on the first page.

%\acmSubmissionID{???}

%%% Use this command to specify the title of your paper.

\title[Multiagent Systems: Homework 2]{Autonomous Agents introduction and Reinforcement Learning}

%%% Provide names, affiliations, and email addresses for all authors.

\author{Connor Kurtz}
\affiliation{
  \institution{Oregon State University}
  \city{Corvallis, OR}}
\email{kurtzco@oregonstate.edu}

%%% Use this environment to specify a short abstract for your paper.

\begin{abstract}
I implement a simple game of tag, and a Q-learning implementation of 
reinforcement learning to teach 1 to 2 players how to catch the target.

\end{abstract}

%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{Multiagent Systems, Reinforcement Learning, Q-learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
\newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
The game being used to study Autonomous Agents and Reinforcement Learning
is a simple game of tag. There are 1-2 players who start at a certain
location in the 5x10 gridworld. There is additionally a target in the 
gridworld that can randomly move 1 square each turn. The players must 
move 1 square each round and their goal is to catch the target. If a
player steps on the same tile as the target, it is awarded with 30 points.
Otherwise, it is punished with a reward of -1 points. 

Three different scenarios are tested in this game. The first is where there 
exists a single player. The second scenario adds another player for a
total of two. The final one augments the reward system so the players share
a combined score value.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Q-learning setup}

I chose to go with Reinforcement learning as opposed to a neural 
network for this, and specifically I chose Q-learning as it seemed 
like the most common method. The state of the system is represented 
with the location of each agent within the gridworld. I consider both
the target and the players to all be agents, so each player and see 
each other and the target at all times.

The state is transformed into an integer to index the Q-table with. The 
Q-table is a $Nx4$ table of floats, where $N=(GRID\_X * GRID\_Y)^
{num\_agents}$. There are four entries per state corresponding to the 
four possible moves: Up, Down, Left, and Right. The Q-value of each input 
is initialized to zero. 

The policy will choose to either pick a random input or the best input 
from the Q-table. It chooses a random input with $\epsilon$ probability.
Otherwise, it converts its current state to an index, and picks the input 
with the highest Q-value. I chose an $\epsilon$ of 0.2, but this value can be 
played with in future work to improve convergence rate.

The Q-table updates are performed as per the Q-learning standard: 
$Q(state, input) = Q(state, input) + StepSize * (Reward + \gamma*max(Q(next\_state)) - Q(state, input))$
The step size was chosen to be 0.8 and $\gamma$ as 0.9. These can be tweaked 
like $\epsilon$ to improve convergence, but this scenario did not seem to 
require that.

Cycles were chosen to end whenever any of the players reached the goal as 
a simple termination. The only meaning of the end of an cycle is the players 
and goal would be reset back to their initial positions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}

I began by running my implementation of the game with 1 and 2 players who
moved at random to generate a baseline. With 1 player, it took on average 
$59$ moves to reach the goal, and with 2 players it took on average $33$ 
moves for one of them to reach the goal.

In the case of using 1 player with Q-learning turned on, it starts out 
much worse than just random selection, taking over 60 moves for the first 
$100$ or so cycles. After that, it quickly overtakes random moves and 
stabilizes at about $40$ moves to reach the goal after about $1500$ cycles.

When adding a second player, it initially takes them over $40$ turns to 
reach the goal for the first $1000$ cycles. The baseline of $33$ moves is
overtaken around $3000$ cycles in, and they stabilize to only needing $20$
moves on average after about $20000$ cycles.

For the scenario where there is a system goal, it seems to impact the players
positively at first, but in the long term it impacts them quite negatively. 
For the first $1000$ or so cycles the players only need between $30$ and $40$
turns to reach the goal, which was better than when they had individual rewards.
However, long term they stabilize at one of them only being able to reach the 
goal within $28$ steps on average after around $3000$ cycles.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Analysis}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}

In conclusion, I implemented a Q-learning reinforcement learning procedure
that could train the players of this simple game to an optimal point within 
roughly $10000$ iterations or fewer. The implementation operated quite quickly, 
being able to reach the optimal point in about 5 to 10 seconds each time. This
assignment was overall very enjoyable to implement from scratch.

A Github repository with my implementation of both the game and the Q-learning 
solver can be found at: 

https://github.com/ckurtz22/MultiagentSystems-HW2


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Citations and References}
  
The use of the \BibTeX\ to prepare your list of references is highly 
recommended. To include the references at the end of your document, put 
the following two commands just before the `\verb|\end{document}|' 
command in your source file:
%
\begin{verbatim}
   \bibliographystyle{ACM-Reference-Format}
   \bibliography{mybibfile}
\end{verbatim}
%
Here we assume that `\texttt{mybibfile.bib}' is the name of your 
\BibTeX\ file. Use the `\verb|\cite|' command to produce citations 
to your references. Here are a few examples for citations of journal 
articles~\cite{GrKr96,WoJe95}, books~\cite{Knu97}, articles in 
conference proceedings~\cite{Hag1993}, technical reports~\cite{Har78},
Master's and PhD theses~\cite{Ani03,Cla85}, online videos~\cite{Oba08}, 
datasets~\cite{AnMC13}, and patents~\cite{Sci09}. Both citations and 
references are numbered by default. 

Make sure you provide complete and correct bibliographic information 
for all your references, and list authors with their full names 
(``Donald E.\ Knuth'') rather than just initials (``D.\ E.\ Knuth''). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The acknowledgments section is defined using the "acks" environment
%%% (rather than an unnumbered section). The use of this environment 
%%% ensures the proper identification of the section in the article 
%%% metadata as well as the consistent spelling of the heading.

\begin{acks}
If you wish to include any acknowledgments in your paper (e.g., to 
people or funding agencies), please do so using the `\texttt{acks}' 
environment. Note that the text of your acknowledgments will be omitted
if you compile your document with the `\texttt{anonymous}' option.
\end{acks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The next two lines define, first, the bibliography style to be 
%%% applied, and, second, the bibliography file to be used.

\bibliographystyle{ACM-Reference-Format} 
\bibliography{sample}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


