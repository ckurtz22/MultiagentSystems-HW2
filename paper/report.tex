%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2021 (based on sample-sigconf.tex)
%%% Prepared by Natasha Alechina and Ulle Endriss (version 2020-08-06)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.
%%% Use the first variant below for the final paper.
%%% Use the second variant below for submission.

\documentclass[sigconf]{aamas} 
%\documentclass[sigconf,anonymous]{aamas} 

%%% Load required packages here (note that many are included already).

\usepackage{balance} % for balancing columns on the final page

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2021 copyright block (do not change!)

%\setcopyright{ifaamas}
%\acmConference[AAMAS '21]{Proc.\@ of the 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2021)}{May 3--7, 2021}{London, UK}{U.~Endriss, A.~Now\'{e}, F.~Dignum, A.~Lomuscio (eds.)}
%\copyrightyear{2021}
%\acmYear{2021}
%\acmDOI{}
%\acmPrice{}
%\acmISBN{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Use this command to specify your EasyChair submission number.
%%% In anonymous mode, it will be printed on the first page.

%\acmSubmissionID{???}

%%% Use this command to specify the title of your paper.

\title[Multiagent Systems: Homework 2]{Autonomous Agents introduction and Reinforcement Learning}

%%% Provide names, affiliations, and email addresses for all authors.

\author{Connor Kurtz}
\affiliation{
  \institution{Oregon State University}
  \city{Corvallis, OR}}
\email{kurtzco@oregonstate.edu}

%%% Use this environment to specify a short abstract for your paper.

\begin{abstract}
I implement a simple game of tag, and a Q-learning implementation of 
reinforcement learning to teach 1 to 2 players how to catch a moving target.

\end{abstract}

%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{Multiagent Systems, Reinforcement Learning, Q-learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
\newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
The game being used to study Autonomous Agents and Reinforcement Learning
is a simple game of tag. There are 1-2 players who start at a certain
location in the 5x10 gridworld. There is additionally a target in the 
gridworld that can randomly move 1 square each turn. The players must 
move 1 square each round and their goal is to catch the target. If a
player steps on the same tile as the target, it is awarded with 30 points.
Otherwise, it is punished with a reward of -1 points. 

Three different scenarios are tested in this game. The first is where there 
exists a single player. The second scenario adds another player for a
total of two. The final one augments the reward system so the players share
a combined score value.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Q-learning setup}

I chose to go with Reinforcement learning as opposed to a neural 
network for this, and specifically I chose Q-learning as it seemed 
like the most common method. The state of the system is represented 
with the location of each agent within the gridworld. I consider both
the target and the players to all be agents, so each player and see 
each other and the target at all times. \cite{SimpleQlearning}~was quite
helpful as a resource for me to reference for implementing the algorithm,
as well as the lecture recordings from class.

The state is transformed into an integer to index the Q-table with. The 
Q-table is a $Nx4$ table of floats, where $N=(GRID\_X * GRID\_Y)^
{num\_agents}$. There are four entries per state corresponding to the 
four possible moves: Up, Down, Left, and Right. The Q-value of each input 
is initialized to zero. 

The policy will choose to either pick a random input or the best input 
from the Q-table. It chooses a random input with $\epsilon$ probability.
Otherwise, it converts its current state to an index, and picks the input 
with the highest Q-value. I chose an $\epsilon$ of 0.2, but this value can be 
played with in future work to improve convergence rate.

The Q-table updates are performed as per the Q-learning standard: 
$Q(state, input) = Q(state, input) + StepSize * (Reward + \gamma*max(Q(next\_state)) - Q(state, input))$
The step size was chosen to be 0.8 and $\gamma$ as 0.9. These can be tweaked 
like $\epsilon$ to improve convergence, but this scenario did not seem to 
require that.

Cycles were chosen to end whenever any of the players reached the goal as 
a simple termination. The only meaning of the end of an cycle is the players 
and goal would be reset back to their initial positions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}

I began by running my implementation of the game with 1 and 2 players who
moved at random to generate a baseline. With 1 player, it took on average 
$59$ moves to reach the goal, and with 2 players it took on average $33$ 
moves for one of them to reach the goal.

In the case of using 1 player with Q-learning turned on, it starts out 
much worse than just random selection, taking over 60 moves for the first 
$100$ or so cycles. After that, it quickly overtakes random moves and 
stabilizes at about $40$ moves to reach the goal after about $1500$ cycles.

When adding a second player, it initially takes them over $40$ turns to 
reach the goal for the first $1000$ cycles. The baseline of $33$ moves is
overtaken around $3000$ cycles in, and they stabilize to only needing $20$
moves on average after about $20000$ cycles.

For the scenario where there is a system goal, it seems to overall impact 
the players slightly negatively. It takes this system around $2000$ cycles 
to improve past $40$ steps to reach the target on average, as opposed to 
$1000$ for the individual reward system. Performance gains seem to stagnate 
at around $23$ steps by about $20000$ iterations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Analysis}

With a single player it's quite hard to tell what the agent is doing. Even though
it is doing fairly well, at least compared to the baseline of random inputs, the 
player still seems erratic in behavior. Sometimes it seems it is somewhat shadowing 
the target, but oftentimes it will just sporadically move away from the target.

When using two players it is also pretty hard to see what is going on. It does seem 
like there might be some implicit cooperation, as sometimes I will notice the agents 
appearing to try and "corner" the target and prevent it from slipping out, but a lot 
of the time one of the players will just seemingly run to the opposite side of the map 
while the other one chases down the target. It's possible this could be viewed as a 
form of cooperation though, as perhaps the players are not sure where exactly the target 
is and are trying to cover more of the map at once. The players do clearly get quite 
good at the game though, being pretty consistent that one of them catches the target 
quite early.

Adding the system reward which ties together both players was very interesting to 
observe. Initially I had actually had a bug in my code where the players were 
receiving a free 30 points every time step due to my implementation treating the target
as an agent as well. This actually had a positive impact on the players for earlier cycles,
but their overall performance was much more limited than individual rewards, only allowing 
the players to improve from around 40 steps on average at worst to about 27 steps after 
20000 cycles. 

Once I fixed this bug, the performance of the two players with a shared system reward
seemed to follow when they had unique rewards, only with requiring slightly more steps
on average each cycle. The players definitely do not seem to benefit from sharing the 
reward between them as overall it decreases performance. This is likely because the 
player who does not catch the target will get a massive reward for doing something that
is likely quite random, enforcing that behavior for no reason.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}

In conclusion, I implemented a Q-learning reinforcement learning procedure
that could train the players of this simple game to an optimal point within 
roughly $10000$ iterations or fewer. The implementation operated quite quickly, 
being able to reach the optimal point in about 5 to 10 seconds each time. This
assignment was overall very enjoyable to implement from scratch.

A Github repository with my implementation of both the game and the Q-learning 
solver can be found at: 

https://github.com/ckurtz22/MultiagentSystems-HW2


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The next two lines define, first, the bibliography style to be 
%%% applied, and, second, the bibliography file to be used.

\bibliographystyle{ACM-Reference-Format} 
\bibliography{report}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


